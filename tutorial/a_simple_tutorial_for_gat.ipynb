{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "141fecd3",
   "metadata": {},
   "source": [
    "# GAT 网络\n",
    "\n",
    "以下实现一个多头注意力图神经网络，是对论文 《Graph Attention Networks》Petar Veličković, et al., 2018的复现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f80f4819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    " \n",
    "class GATLayer(nn.Module):\n",
    " \n",
    "    def __init__(self,c_in,c_out,\n",
    "                num_heads=1, concat_heads=True, alpha=0.2):\n",
    "        \"\"\"\n",
    "        :param c_in: 输入特征维度\n",
    "        :param c_out: 输出特征维度\n",
    "        :param num_heads: 多头的数量\n",
    "        :param concat_heads: 是否拼接多头计算的结果\n",
    "        :param alpha: LeakyReLU的参数\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.concat_heads = concat_heads\n",
    "        if self.concat_heads:\n",
    "            assert c_out % num_heads ==0,\"输出特征数必须是头数的倍数！\"\n",
    "            c_out = c_out // num_heads\n",
    " \n",
    "        #参数\n",
    "        self.projection = nn.Linear(c_in,c_out*num_heads) #有几个头，就需要将c_out扩充几倍\n",
    "        self.a = nn.Parameter(torch.Tensor(num_heads,2*c_out)) #用于计算注意力的参数，由于对两节点拼接后的向量进行操作，所以2*c_out\n",
    "        self.leakrelu = nn.LeakyReLU(alpha) #激活层\n",
    " \n",
    "        #参数初始化\n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414) # 近似sqrt(2)\n",
    " \n",
    "    def forward(self,node_feats,adj_matrix,print_attn_probs=False):\n",
    "        \"\"\"\n",
    "        输入：\n",
    "        :param self:\n",
    "        :param node_feats: 节点的特征表示\n",
    "        :param adj_matrix: 邻接矩阵\n",
    "        :param print_attn_probs: 是否打印注意力\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        batch_size,num_nodes = node_feats.size(0),node_feats.size(1)\n",
    "        # batch_size是节点的特征维度，num_nodes是节点数\n",
    "        #将节点初始输入进行权重运算\n",
    "        node_feats = self.projection(node_feats)\n",
    "        #扩展出多头数量的维度\n",
    "        node_feats = node_feats.view(batch_size,num_nodes,self.num_heads,-1)\n",
    "\n",
    "        # 获取所有顶点对拼接而成的特征向量 a_input\n",
    "        edges = adj_matrix.nonzero(as_tuple=False)  # 返回所有邻接矩阵中值不为 0 的 index，即所有连接的边对应的两个顶点\n",
    "        node_feats_flat = node_feats.view(batch_size * num_nodes, self.num_heads, -1)  # 将所有 batch_size 的节点拼接\n",
    "\n",
    "        edge_indices_row = edges[:, 0] * num_nodes  + edges[:, 1]  # 获取边对应的第一个顶点 index\n",
    "        edge_indices_col = edges[:, 0] * num_nodes  + edges[:, 2]  # 获取边对应的第二个顶点 index\n",
    "\n",
    "        a_input = torch.cat([\n",
    "        torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0), # 基于边对应的第一个顶点的 index 获取其特征值\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0)  # 基于边对应的第二个顶点的 index 获取其特征值\n",
    "        ], dim=-1)  # 两者拼接\n",
    "\n",
    "        # 基于权重 a 进行注意力计算\n",
    "        attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a)\n",
    "        # LeakyReLU 计算\n",
    "        attn_logits = self.leakrelu(attn_logits)\n",
    "\n",
    "        # 将注意力权转换为矩阵的形式\n",
    "        attn_matrix = attn_logits.new_zeros(adj_matrix.shape + (self.num_heads,)).fill_(-9e15)\n",
    "        attn_matrix[adj_matrix[..., None].repeat(1, 1, 1, self.num_heads) == 1] = attn_logits.reshape(-1)\n",
    "\n",
    "        # Softmax 计算转换为概率\n",
    "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
    "        if print_attn_probs:\n",
    "            print(\"注意力权重:\\n\", attn_probs.permute(0, 3, 1, 2))\n",
    "        # 对每个节点进行注意力加权相加的计算\n",
    "        node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
    "\n",
    "        # 根据是否将多头的计算结果拼接与否进行不同操作\n",
    "        if self.concat_heads:  # 拼接\n",
    "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
    "        else:  # 平均\n",
    "            node_feats = node_feats.mean(dim=2)\n",
    "\n",
    "        return node_feats "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931668aa",
   "metadata": {},
   "source": [
    "举一个例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c7df1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注意力权重:\n",
      " tensor([[[[0.3543, 0.6457, 0.0000, 0.0000],\n",
      "          [0.1096, 0.1450, 0.2642, 0.4813],\n",
      "          [0.0000, 0.1858, 0.2885, 0.5257],\n",
      "          [0.0000, 0.2391, 0.2696, 0.4913]],\n",
      "\n",
      "         [[0.5100, 0.4900, 0.0000, 0.0000],\n",
      "          [0.2975, 0.2436, 0.2340, 0.2249],\n",
      "          [0.0000, 0.3838, 0.3142, 0.3019],\n",
      "          [0.0000, 0.4018, 0.3289, 0.2693]]]])\n",
      "节点特征:\n",
      " tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "添加自连接的邻接矩阵:\n",
      " tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n",
      "节点输出特征:\n",
      " tensor([[[1.2913, 1.9800],\n",
      "         [4.2344, 3.7725],\n",
      "         [4.6798, 4.8362],\n",
      "         [4.5043, 4.7351]]])\n"
     ]
    }
   ],
   "source": [
    "layer = GATLayer(2, 2, num_heads=2)\n",
    "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
    "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
    "layer.a.data = torch.Tensor([[-0.2, 0.3], [0.1, -0.1]])\n",
    "node_feats = torch.arange(8, dtype=torch.float32).view(1, 4, 2)\n",
    "adj_matrix = torch.Tensor([[[1, 1, 0, 0],\n",
    "                                    [1, 1, 1, 1],\n",
    "                                    [0, 1, 1, 1],\n",
    "                                    [0, 1, 1, 1]]])\n",
    "with torch.no_grad():\n",
    "    out_feats = layer(node_feats, adj_matrix, print_attn_probs=True)\n",
    "\n",
    "\n",
    "print(\"节点特征:\\n\", node_feats)\n",
    "print(\"添加自连接的邻接矩阵:\\n\", adj_matrix)\n",
    "print(\"节点输出特征:\\n\", out_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e245f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3, 1], edge_index=[2, 4])\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
